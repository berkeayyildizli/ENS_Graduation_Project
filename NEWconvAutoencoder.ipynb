{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8f9a777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sample frame loaded: Frame_46.csv\n",
      "🖼️ Image dimensions: 200 (height) × 201 (width)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to your image folder\n",
    "frames_dir = 'data/images'\n",
    "\n",
    "# Get a sample frame file\n",
    "sample_file = next((f for f in os.listdir(frames_dir) if f.endswith('.csv') and f.startswith('Frame_')), None)\n",
    "\n",
    "if sample_file:\n",
    "    sample_path = os.path.join(frames_dir, sample_file)\n",
    "    sample_frame = pd.read_csv(sample_path, header=None)\n",
    "    print(f\"✅ Sample frame loaded: {sample_file}\")\n",
    "    print(f\"🖼️ Image dimensions: {sample_frame.shape[0]} (height) × {sample_frame.shape[1]} (width)\")\n",
    "else:\n",
    "    print(\"❌ No valid .csv frame files found in the directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3fefba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 873ms/step - loss: 0.6570 - val_loss: 0.6065\n",
      "Epoch 2/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 831ms/step - loss: 0.5547 - val_loss: 0.6045\n",
      "Epoch 3/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 810ms/step - loss: 0.5543 - val_loss: 0.5935\n",
      "Epoch 4/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 754ms/step - loss: 0.5534 - val_loss: 0.5881\n",
      "Epoch 5/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 765ms/step - loss: 0.5546 - val_loss: 0.5716\n",
      "Epoch 6/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 750ms/step - loss: 0.5545 - val_loss: 0.5660\n",
      "Epoch 7/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 753ms/step - loss: 0.5546 - val_loss: 0.5615\n",
      "Epoch 8/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 754ms/step - loss: 0.5531 - val_loss: 0.5605\n",
      "Epoch 9/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 755ms/step - loss: 0.5505 - val_loss: 0.5602\n",
      "Epoch 10/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 769ms/step - loss: 0.5533 - val_loss: 0.5619\n",
      "Epoch 11/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 753ms/step - loss: 0.5530 - val_loss: 0.5606\n",
      "Epoch 12/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 752ms/step - loss: 0.5526 - val_loss: 0.5603\n",
      "Epoch 13/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 748ms/step - loss: 0.5511 - val_loss: 0.5605\n",
      "Epoch 14/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 773ms/step - loss: 0.5529 - val_loss: 0.5650\n",
      "Epoch 15/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 780ms/step - loss: 0.5527 - val_loss: 0.5602\n",
      "Epoch 16/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 760ms/step - loss: 0.5517 - val_loss: 0.5614\n",
      "Epoch 17/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 760ms/step - loss: 0.5544 - val_loss: 0.5609\n",
      "Epoch 18/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 777ms/step - loss: 0.5534 - val_loss: 0.5609\n",
      "Epoch 19/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 782ms/step - loss: 0.5512 - val_loss: 0.5659\n",
      "Epoch 20/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 753ms/step - loss: 0.5535 - val_loss: 0.5603\n",
      "Epoch 21/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 753ms/step - loss: 0.5548 - val_loss: 0.5926\n",
      "Epoch 22/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 756ms/step - loss: 0.5533 - val_loss: 0.5603\n",
      "Epoch 23/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 763ms/step - loss: 0.5522 - val_loss: 0.5599\n",
      "Epoch 24/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 757ms/step - loss: 0.5482 - val_loss: 0.5601\n",
      "Epoch 25/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 762ms/step - loss: 0.5527 - val_loss: 0.5611\n",
      "Epoch 26/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 750ms/step - loss: 0.5538 - val_loss: 0.5641\n",
      "Epoch 27/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 762ms/step - loss: 0.5530 - val_loss: 0.5602\n",
      "Epoch 28/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 902ms/step - loss: 0.5529 - val_loss: 0.5608\n",
      "Epoch 29/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 742ms/step - loss: 0.5514 - val_loss: 0.5603\n",
      "Epoch 30/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 749ms/step - loss: 0.5495 - val_loss: 0.5602\n",
      "Epoch 31/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 771ms/step - loss: 0.5543 - val_loss: 0.5610\n",
      "Epoch 32/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 786ms/step - loss: 0.5534 - val_loss: 0.5601\n",
      "Epoch 33/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 769ms/step - loss: 0.5508 - val_loss: 0.5603\n",
      "Epoch 34/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 748ms/step - loss: 0.5546 - val_loss: 0.5609\n",
      "Epoch 35/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 748ms/step - loss: 0.5520 - val_loss: 0.5632\n",
      "Epoch 36/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 755ms/step - loss: 0.5533 - val_loss: 0.5600\n",
      "Epoch 37/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 754ms/step - loss: 0.5513 - val_loss: 0.5602\n",
      "Epoch 38/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 759ms/step - loss: 0.5513 - val_loss: 0.5617\n",
      "Epoch 39/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 766ms/step - loss: 0.5505 - val_loss: 0.5602\n",
      "Epoch 40/40\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 833ms/step - loss: 0.5506 - val_loss: 0.5597\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 322ms/step\n",
      "✅ Thresholds applied. Results saved to 'conv_autoencoder_results.csv'\n",
      "95th: 0.001600, 99th: 0.002109\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Cropping2D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Set frame dimensions\n",
    "frame_height, frame_width = 200, 201\n",
    "\n",
    "# === Step 1: Load and preprocess data ===\n",
    "frames_dir = 'data/images'\n",
    "frame_files = sorted([f for f in os.listdir(frames_dir) if f.startswith('Frame_') and f.endswith('.csv')])\n",
    "\n",
    "all_data = []\n",
    "for file_name in frame_files:\n",
    "    file_path = os.path.join(frames_dir, file_name)\n",
    "    frame = pd.read_csv(file_path, header=None).values.astype(np.float32)\n",
    "    frame[frame == 0] = np.nan  # Treat 0s as missing\n",
    "    all_data.append(frame)\n",
    "\n",
    "all_data = np.array(all_data)  # Shape: (num_samples, 200, 201)\n",
    "\n",
    "# Impute and normalize each frame\n",
    "for i in range(all_data.shape[0]):\n",
    "    frame = all_data[i]\n",
    "    nan_mask = np.isnan(frame)\n",
    "    if np.any(nan_mask):\n",
    "        col_mean = np.nanmean(frame, axis=0)\n",
    "        frame[nan_mask] = np.take(col_mean, np.where(nan_mask)[1])\n",
    "    all_data[i] = frame\n",
    "\n",
    "# Scale all values to [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "reshaped_data = all_data.reshape(all_data.shape[0], -1)  # Flatten for scaler\n",
    "reshaped_data = scaler.fit_transform(reshaped_data)\n",
    "num_frames = all_data.shape[0]\n",
    "all_data = reshaped_data.reshape(num_frames, frame_height, frame_width, 1)\n",
    "\n",
    "# === Step 2: Define convolutional autoencoder ===\n",
    "def create_conv_autoencoder(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    from keras.layers import Dropout, BatchNormalization\n",
    "\n",
    "    # Encoder\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "    #? Dropout layer \n",
    "    \n",
    "        # Decoder\n",
    "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(encoded)\n",
    "    x = UpSampling2D((2, 2))(x)  # -> 100x102\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)  # -> 200x204\n",
    "    x = Cropping2D(cropping=((0, 0), (0, 3)))(x)  # -> 200x201\n",
    "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    autoencoder.compile(optimizer=Adam(), loss='mse')\n",
    "    return autoencoder\n",
    "\n",
    "input_shape = (frame_height, frame_width, 1)\n",
    "autoencoder = create_conv_autoencoder(input_shape)\n",
    "\n",
    "# === Step 3: Train the model ===\n",
    "autoencoder.fit(all_data, all_data,\n",
    "                epochs=50,\n",
    "                batch_size=16,\n",
    "                shuffle=True,\n",
    "                verbose=1)\n",
    "\n",
    "# === Step 4: Calculate reconstruction errors ===\n",
    "reconstructions = autoencoder.predict(all_data)\n",
    "recon_errors = []\n",
    "\n",
    "for original, reconstructed, fname in zip(all_data, reconstructions, frame_files):\n",
    "    mse = mean_squared_error(original.flatten(), reconstructed.flatten())\n",
    "    recon_errors.append((fname, mse))\n",
    "\n",
    "mse_df = pd.DataFrame(recon_errors, columns=[\"Frame\", \"MSE\"])\n",
    "\n",
    "# === Step 5: Thresholding ===\n",
    "threshold_99 = np.percentile(mse_df['MSE'], 99)\n",
    "threshold_95 = np.percentile(mse_df['MSE'], 95)\n",
    "\n",
    "mse_df['Predicted_99'] = mse_df['MSE'].apply(lambda x: 1 if x > threshold_99 else 0)\n",
    "mse_df['Predicted_95'] = mse_df['MSE'].apply(lambda x: 1 if x > threshold_95 else 0)\n",
    "\n",
    "# Save results\n",
    "mse_df.to_csv(\"conv_autoencoder_results.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Threshold (95th percentile): {threshold_95:.6f}\")\n",
    "print(f\"✅ Threshold (99th percentile): {threshold_99:.6f}\")\n",
    "print(f\"📄 Results saved to 'conv_autoencoder_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e4c5e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Evaluation at 90th Percentile Threshold:\n",
      "F1 Score: 0.5254237288135594\n",
      "Confusion Matrix:\n",
      " [[1477   16]\n",
      " [  40   31]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      1493\n",
      "           1       0.66      0.44      0.53        71\n",
      "\n",
      "    accuracy                           0.96      1564\n",
      "   macro avg       0.82      0.71      0.75      1564\n",
      "weighted avg       0.96      0.96      0.96      1564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "# Recalculate 90th percentile threshold\n",
    "threshold_90 = np.percentile(mse_df['MSE'], 97)\n",
    "mse_df['Predicted_90'] = mse_df['MSE'].apply(lambda x: 1 if x > threshold_90 else 0)\n",
    "\n",
    "# Merge again with labels if not already done\n",
    "labels_path = 'data/frame_porosity_labels.csv'\n",
    "labels_df = pd.read_csv(labels_path)\n",
    "labels_df.columns = labels_df.columns.str.strip()\n",
    "labels_df['Frame'] = labels_df['Frame'].astype(str).apply(lambda x: f\"Frame_{x}\")\n",
    "mse_df['Frame'] = mse_df['Frame'].str.replace('.csv', '', regex=False)\n",
    "\n",
    "merged = pd.merge(mse_df, labels_df, on='Frame', how='inner')\n",
    "\n",
    "# Evaluate\n",
    "y_true = merged['Porosity Label']\n",
    "y_pred_90 = merged['Predicted_90']\n",
    "\n",
    "print(\"\\n📊 Evaluation at 90th Percentile Threshold:\")\n",
    "print(\"F1 Score:\", f1_score(y_true, y_pred_90))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred_90))\n",
    "print(\"Classification Report:\\n\", classification_report(y_true, y_pred_90))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
